# ğŸ” LangChain-Debugger

<img width="428" height="260" alt="image" src="https://github.com/user-attachments/assets/5c275341-6d49-4ab8-9220-92a16a8ee826" />


**Visual debugging and optimization tool for LangChain applications**

Monitor, analyze, and optimize your LLM chains in real-time with detailed performance metrics, cost breakdowns, and actionable optimization suggestions.

![Python](https://img.shields.io/badge/python-3.8+-blue.svg)
![LangChain](https://img.shields.io/badge/langchain-0.1+-green.svg)
![License](https://img.shields.io/badge/license-MIT-blue.svg)
 
---
 
## ğŸŒŸ Features

### ğŸ“Š Chain Visualization
- Auto-generate flow diagrams from LangChain code
- Mermaid diagram export for documentation
- Interactive Plotly visualizations
- ASCII diagrams for command-line use

### ğŸ“ˆ Real-time Monitoring
- Track token usage per component
- Measure latency at each step
- Monitor LLM API calls
- Capture complete execution flow

### ğŸ’° Cost Analysis
- Cost breakdown per LLM call
- Estimated costs for 1K, 10K calls
- Monthly cost projections
- Model comparison recommendations

### ğŸ¯ Performance Optimization
- Identify bottlenecks in your chain
- Get actionable suggestions to reduce costs
- Token efficiency scoring
- Model alternatives with savings estimates

### ğŸ› ï¸ Easy Integration
```python
from debugger import DebugMode

with DebugMode() as debugger:
    result = chain.invoke({"input": "..."})
    debugger.print_report()
```

---

## ğŸš€ Quick Start

### Installation

```bash
# Clone the repository
git clone https://github.com/aftabbs/langchain-debugger.git
cd langchain-debugger

# Install dependencies
pip install -r requirements.txt

# Set your API key
export OPENAI_API_KEY="your-key-here"
# or GROQ_API_KEY, ANTHROPIC_API_KEY
```

### Basic Usage

```python
from langchain_core.prompts import ChatPromptTemplate
from langchain_openai import ChatOpenAI
from langchain_core.output_parsers import StrOutputParser
from debugger import DebugMode

# Create your chain
llm = ChatOpenAI(model="gpt-3.5-turbo")
prompt = ChatPromptTemplate.from_template("Tell me a joke about {topic}")
chain = prompt | llm | StrOutputParser()

# Debug it!
with DebugMode() as debugger:
    result = debugger.debug_chain(
        chain,
        inputs={"topic": "programming"}
    )
    debugger.print_report()
```

### Output Example

```
============================================================
               LANGCHAIN DEBUG REPORT
============================================================

=== Chain Structure ===
Type: RunnableSequence
Structure: sequence

Components (3):
  1. ChatPromptTemplate (prompt)
  2. ChatOpenAI (llm)
     â€¢ model: gpt-3.5-turbo
     â€¢ temperature: 0.7
  3. StrOutputParser (parser)

=== Performance Analysis ===
Total Latency: 1.23s
LLM Calls: 1
Avg Latency/Call: 1.23s

=== Cost Analysis ===
Total Cost: $0.000300
Cost per Call: $0.000300
Est. 1K calls: $0.30
Est. Monthly (30K): $9.00

=== Token Efficiency ===
Prompt Tokens: 45
Completion Tokens: 105
Total Tokens: 150
Efficiency Score: Good

=== Optimization Suggestions ===
1. ğŸŸ¢ âœ… Prompt length (45 tokens) is well optimized!
2. ğŸŸ¡ ğŸ—„ï¸ Implement prompt caching for repeated queries
   â†’ Potential savings: 50-90%
3. ğŸŸ¢ âš¡ For ultra-fast inference, consider Groq API
   â†’ Potential savings: 90% latency reduction
============================================================
```

---

## ğŸ“– Usage Examples

### Example 1: Quick Debug (One-liner)

```python
from debugger import debug_chain

result = debug_chain(chain, inputs={"topic": "AI"})
```

### Example 2: Inspect Chain Without Execution

```python
debugger = DebugMode(verbose=True)
debugger.debug_chain(chain)  # No inputs = structure only
print(debugger.get_mermaid_diagram())
```

### Example 3: Compare Model Costs

```python
models = ["gpt-3.5-turbo", "gpt-4", "gpt-4-turbo"]

for model_name in models:
    llm = ChatOpenAI(model=model_name)
    chain = prompt | llm | StrOutputParser()
    
    debugger = DebugMode(verbose=False)
    debugger.debug_chain(chain, inputs={"topic": "AI"})
    
    cost = debugger.analysis['cost']['total_cost']
    print(f"{model_name}: ${cost:.6f}")
```

### Example 4: Export Mermaid Diagram

```python
with DebugMode() as debugger:
    debugger.debug_chain(chain)
    
    # Get Mermaid syntax
    mermaid = debugger.get_mermaid_diagram()
    
    # Save to file
    with open("chain_diagram.mmd", "w") as f:
        f.write(mermaid)
    
    # Open in mermaid.live for visualization
```

---

## ğŸ–¥ï¸ Interactive Dashboard

Launch the Gradio web interface for visual debugging:

```bash
python ui/dashboard.py
```

Then open http://localhost:7860 in your browser.

**Dashboard Features:**
- ğŸ›ï¸ Configure multiple LLM providers (OpenAI, Anthropic, Groq)
- ğŸ§ª Test different chain types
- ğŸ“Š Real-time performance visualization
- ğŸ’¡ Interactive optimization suggestions
- ğŸ“ˆ Cost comparison tools

---

## ğŸ“ Project Structure

```
langchain-debugger/
â”œâ”€â”€ debugger/
â”‚   â”œâ”€â”€ __init__.py          # Main DebugMode interface
â”‚   â”œâ”€â”€ inspector.py         # Chain structure introspection
â”‚   â”œâ”€â”€ monitor.py           # Runtime execution monitoring
â”‚   â”œâ”€â”€ analyzer.py          # Performance analysis
â”‚   â””â”€â”€ visualizer.py        # Diagram generation
â”œâ”€â”€ ui/
â”‚   â””â”€â”€ dashboard.py         # Gradio web interface
â”œâ”€â”€ examples/
â”‚   â”œâ”€â”€ demo_chains.py       # Example chain builders
â”‚   â””â”€â”€ cli_examples.py      # Command-line examples
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
```

---

## ğŸ”§ Supported Features

### Chain Types
- âœ… Sequential chains (RunnableSequence)
- âœ… Parallel chains (RunnableParallel)
- âœ… Single component chains
- âœ… Multi-step chains
- âœ… Nested chains

### LLM Providers
- âœ… OpenAI (GPT-3.5, GPT-4)
- âœ… Anthropic (Claude)
- âœ… Groq (Llama, Mixtral)
- âœ… Any LangChain-compatible LLM

### Output Formats
- âœ… Console text report
- âœ… ASCII diagrams
- âœ… Mermaid diagrams
- âœ… Plotly interactive graphs
- âœ… JSON export

---

## ğŸ’¡ Use Cases

### 1. Development & Debugging
- Understand complex chain structures
- Identify which components are slow
- Debug unexpected behavior

### 2. Cost Optimization
- Find expensive LLM calls
- Compare model pricing
- Optimize token usage

### 3. Performance Tuning
- Identify latency bottlenecks
- Test parallel vs sequential execution
- Benchmark different models

### 4. Documentation
- Generate visual chain diagrams
- Export architecture documentation
- Share chain designs with team

---

## ğŸ¯ Optimization Suggestions Examples

The debugger provides intelligent suggestions:

**High Priority:**
- ğŸ’° "High cost per call ($0.05) - consider cheaper models or caching"
- â±ï¸ "Total latency is 5.2s - consider streaming or parallel execution"
- ğŸ“ "Prompt is 1500 tokens - consider summarizing or using RAG"

**Medium Priority:**
- ğŸ’¡ "Consider using GPT-4 Turbo instead of GPT-4 for 67% cost savings"
- ğŸ”„ "Chain makes 5 LLM calls - consider combining prompts"

**Low Priority:**
- âš¡ "For ultra-fast inference, consider Groq API - 10x faster"
- âœ… "Prompt length (45 tokens) is well optimized!"

---

## ğŸ¤ Contributing

Contributions are welcome! This tool is built for the LangChain community.

**Ways to contribute:**
- ğŸ› Report bugs
- ğŸ’¡ Suggest features
- ğŸ“ Improve documentation
- ğŸ”§ Submit pull requests

---

## ğŸ“ Requirements

- Python 3.8+
- LangChain 0.1.0+
- Gradio 4.0+ (for UI)
- Plotly 5.18+ (for visualizations)

See `requirements.txt` for full list.

---

## ğŸ“ Examples & Tutorials

### Run CLI Examples

```bash
python examples/cli_examples.py
```

This provides 5 interactive examples:
1. Simple chain debugging
2. Chain structure inspection
3. Quick debug (one-liner)
4. Groq fast inference comparison
5. Cost comparison across models

### Import Demo Chains

```python
from examples.demo_chains import (
    get_simple_chain,
    get_complex_chain,
    get_parallel_chain,
    get_json_output_chain,
    get_multi_step_analysis_chain
)

# Use any demo chain
chain = get_simple_chain(llm)
```

---

## ğŸš¦ Getting Started Checklist

- [ ] Install dependencies: `pip install -r requirements.txt`
- [ ] Set API key: `export OPENAI_API_KEY="..."`
- [ ] Run basic example: `python examples/cli_examples.py`
- [ ] Try web dashboard: `python ui/dashboard.py`
- [ ] Debug your own chain
- [ ] Optimize based on suggestions

---

## ğŸ”’ Privacy & Security

- **No data storage**: All debugging happens locally
- **API keys**: Never logged or stored
- **Chain data**: Only in memory during execution
- **Open source**: Review the code yourself

---

## ğŸ“„ License

MIT License - see LICENSE file for details.

---

## â­ Star History

If you find this tool useful, please consider giving it a star! â­

---

**Made with ğŸ” by the LangChain community**
